Jasne, Martyna! UporzÄ…dkujmy tÄ™ wiedzÄ™, aby byÅ‚a maksymalnie przejrzysta i Å‚atwa do przyswojenia.

PojÄ™cie **Batcha** (czÄ™sto tÅ‚umaczone jako "wsad" lub "partia") jest fundamentalne dla treningu sieci neuronowych. PoniÅ¼ej znajdziesz sformatowane i czytelne wyjaÅ›nienie tego mechanizmu.

---

# Czym jest Batch? (Wsad danych)

W uczeniu maszynowym **Batch** to po prostu **paczka danych**, ktÃ³rÄ… sieÄ‡ przetwarza jednoczeÅ›nie w jednym kroku obliczeniowym.

Aby to zrozumieÄ‡, posÅ‚uÅ¼my siÄ™ analogiÄ… budowlanÄ…:

> **ğŸ§± Analogia: Przenoszenie 1000 cegieÅ‚ z punktu A do B**
> 
> 1. **PodejÅ›cie pojedyncze (Stochastic):** Nosisz po **jednej** cegle w rÄ™kach.
>     
>     - _Efekt:_ Bardzo wolne, ciÄ…gÅ‚e kursowanie tam i z powrotem.
>         
> 2. **PodejÅ›cie caÅ‚oÅ›ciowe (Full Batch):** PrÃ³bujesz wziÄ…Ä‡ **wszystkie 1000** cegieÅ‚ naraz.
>     
>     - _Efekt:_ NiemoÅ¼liwe â€“ nie masz tyle siÅ‚y (pamiÄ™ci RAM), cegÅ‚y siÄ™ rozsypiÄ….
>         
> 3. **PodejÅ›cie "Batch" (Mini-batch):** Bierzesz taczkÄ™ i wozisz po **32 cegÅ‚y** na raz.
>     
>     - _Efekt:_ **Idealny kompromis**. Wykorzystujesz w peÅ‚ni swoje narzÄ™dzie (taczkÄ™), praca idzie szybko, a Ty siÄ™ nie przeciÄ…Å¼asz.
>         

W tej analogii **taczka to Twoja karta graficzna (GPU)**, a **liczba cegieÅ‚ (32) to `batch_size`**.

---

## 1. Dlaczego nie uczymy na wszystkim naraz?

Podczas treningu sieÄ‡ nie widzi caÅ‚ego zbioru danych (np. 100 000 zdjÄ™Ä‡) w jednej chwili z dwÃ³ch powodÃ³w:

1. **ğŸ’¾ PamiÄ™Ä‡ (RAM/VRAM):** Karta graficzna ma ograniczonÄ… pamiÄ™Ä‡. Nie zmieÅ›ciÅ‚aby milionÃ³w parametrÃ³w sieci ORAZ wszystkich zdjÄ™Ä‡ jednoczeÅ›nie.
    
2. **ğŸš€ SzybkoÅ›Ä‡ (RÃ³wnolegÅ‚oÅ›Ä‡):** GPU to bestie obliczeniowe stworzone do pracy rÃ³wnolegÅ‚ej. Przetworzenie 1 zdjÄ™cia trwa prawie tyle samo, co 32 zdjÄ™Ä‡ naraz. UÅ¼ywanie batchy drastycznie przyspiesza proces.
    

### SÅ‚owniczek pojÄ™Ä‡

- **Batch Size (Rozmiar partii):** Liczba przykÅ‚adÃ³w w jednej paczce (np. 32, 64, 128).
    
- **Iteracja (Krok):** Przetworzenie jednego batcha (Obliczenie bÅ‚Ä™du $\rightarrow$ Aktualizacja wag).
    
- **Epoka:** Moment, w ktÃ³rym sieÄ‡ zobaczyÅ‚a (w kawaÅ‚kach) juÅ¼ **wszystkie** przykÅ‚ady ze zbioru dokÅ‚adnie raz.
    

---

## 2. Operacje na Danych (Pipeline w TensorFlow)

W notatniku `tf.data.Dataset` dziaÅ‚a jak taÅ›ma produkcyjna przygotowujÄ…ca te paczki. Oto co robiÄ… poszczegÃ³lne funkcje:

### A. `batch(size)` â€“ Tworzenie paczek

To najwaÅ¼niejsza operacja techniczna. Zmienia ona wymiary danych (tensora).

- **Co robi:** Skleja pojedyncze przykÅ‚ady w grupy.
    
- **Zmiana ksztaÅ‚tu:**
    
    - Pojedyncze zdjÄ™cie: `(28, 28, 3)` $\rightarrow$ (WysokoÅ›Ä‡, SzerokoÅ›Ä‡, Kolor)
        
    - Po zbatchowaniu: `(32, 28, 28, 3)` $\rightarrow$ (**Indeks w paczce**, WysokoÅ›Ä‡, SzerokoÅ›Ä‡, Kolor)
        
- **Cel:** DziÄ™ki temu GPU moÅ¼e "poÅ‚knÄ…Ä‡" 32 zdjÄ™cia na raz.
    

### B. `shuffle(buffer_size)` â€“ Tasowanie

Krytyczne dla jakoÅ›ci nauki.

- **Problem:** JeÅ›li dane sÄ… posortowane (np. najpierw 1000 zdjÄ™Ä‡ kotÃ³w, potem 1000 psÃ³w), sieÄ‡ w pierwszej fazie nauczy siÄ™, Å¼e "Å›wiat skÅ‚ada siÄ™ tylko z kotÃ³w".
    
- **RozwiÄ…zanie:** `shuffle` bierze grupÄ™ elementÃ³w do bufora i losuje z nich te, ktÃ³re trafiÄ… do batcha.
    
- **Efekt:** W jednym batchu masz wymieszane koty i psy. Uczenie jest **stabilne**, a gradient (kierunek nauki) bardziej wiarygodny.
    

### C. `prefetch(tf.data.AUTOTUNE)` â€“ Przyspieszanie

Optymalizacja wydajnoÅ›ci, dziaÅ‚ajÄ…ca jak dobry kelner.

- **Problem:** GPU liczy szybciej niÅ¼ CPU wczytuje dane z dysku. Bez `prefetch` GPU musiaÅ‚oby czekaÄ‡ na dane ("nuda").
    
- **RozwiÄ…zanie:**
    
    - Gdy GPU "trawi" (trenuje) **Batch nr 1**...
        
    - ...CPU w tle juÅ¼ przygotowuje i Å‚aduje do pamiÄ™ci **Batch nr 2**.
        
- **Efekt:** Zero przestojÃ³w. Utylizacja sprzÄ™tu wynosi 100%.
    

### D. `map()` â€“ Przetwarzanie "w locie"

UÅ¼ywane do augmentacji (obracanie, przycinanie) lub normalizacji.

- **DziaÅ‚anie:** Aplikuje funkcjÄ™ do kaÅ¼dego elementu. DziÄ™ki batchowaniu operacja ta wykonywana jest na wielu elementach rÃ³wnolegle (wektoryzacja), co jest znacznie szybsze niÅ¼ pÄ™tla `for`.
    

---

## 3. Podsumowanie w kodzie

Gdy w kodzie widzisz takÄ… linijkÄ™:

Python

```
dataset = dataset.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)
```

Oznacza to nastÄ™pujÄ…cy proces:

1. ğŸ”€ **Shuffle:** WeÅº 1000 przykÅ‚adÃ³w do worka i wylosuj.
    
2. ğŸ“¦ **Batch:** Sklej wylosowane przykÅ‚ady w paczkÄ™ po 32 sztuki.
    
3. â© **Prefetch:** Gdy model uczy siÄ™ na bieÅ¼Ä…cej paczce, przygotuj juÅ¼ nastÄ™pnÄ… w tle.
    

**Czy ta forma notatki jest dla Ciebie czytelniejsza?**